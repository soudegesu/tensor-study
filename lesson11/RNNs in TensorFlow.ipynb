{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "* All about RNNs\n",
    "* Implementation tricks & treats\n",
    "* Live demo of Language Modeling\n",
    "\n",
    "### 補助資料\n",
    "http://qiita.com/t_Signull/items/21b82be280b46f467d1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From feed-forward to RNNs\n",
    "feed-forward: 目標値(や推定誤差)から（結果を使用せずに）操作量を計算する。目標値から直接操作量を出すので、応答が早い一方で、操作量の計算中の誤差がそのまま計算に反映されてしまう。\n",
    "\n",
    "* RNNはシーケンシャルなデータ(text, 遺伝子, 単語)の利点を得ることができる\n",
    "* Directed cycle(有向閉路グラフ)\n",
    "* すべてのステップはパラメータの総数を減らすために、weightをシェアする\n",
    "* NLPの重要要素である\n",
    "* 画像処理にも使用されることがある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNsの問題点\n",
    "*  long-termな依存関係をキャプチャするのは得意ではない\n",
    "例えば、非常に長いシンボルが続いた後に、その後、最初のxまたはyが出現するという系列を学習する場合、最初の要素の情報を少なくともpステップ維持する機能をもつようにNNの重みを更新する必要がある。数十ステップなら可能であるが、数千ステップのような長期の系列は学習ができなかった。\n",
    "\n",
    "そのため、、\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMの流行\n",
    "**Long Short Term Memory**\n",
    "\n",
    "Long tem memory(長期記憶)とShort term memory(短期記憶)という神経科学における用語から取られている。\n",
    "LSTMは **RNNの中間層のユニットをLSTM blockと呼ばれるメモリと3つのゲートを持つブロックに置き換える** ことで実現している\n",
    "\n",
    "* 3つのゲート\n",
    "    * Input Gate\n",
    "    * Forget Gate\n",
    "    * Output/Exposure Gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9486.994140625. Time 10.257993936538696\n",
      "T=ii  e e e  e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e \n",
      "Iter 79. \n",
      "    Loss 8290.7099609375. Time 9.71593713760376\n",
      "Twon the ter the ter the ter te ter te ter te ter te ter te ter te ter te ter te ter te ter te ter te ter an the ter an the ter te ter te ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an the ter an th\n",
      "Iter 119. \n",
      "    Loss 7533.07958984375. Time 9.670512199401855\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 159. \n",
      "    Loss 7002.18505859375. Time 9.480427980422974\n",
      "The the seder and the the seder and the the seder and the the seder and the the sed the the sed the the sed the the sed the the sed the the sed the the sed the the sed the the sed the sed the the sed the the sed the the sed the the sed the the sed the the sed the the for ard the the sed the the for a\n",
      "Iter 199. \n",
      "    Loss 6448.029296875. Time 9.545055150985718\n",
      "The senting ation the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting and the converting an\n",
      "Iter 239. \n",
      "    Loss 6178.8623046875. Time 9.45331597328186\n",
      "The semples and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 279. \n",
      "    Loss 6035.78955078125. Time 9.401798963546753\n",
      "The propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a propesting the propesting a prop\n",
      "Iter 319. \n",
      "    Loss 6223.9716796875. Time 9.591969013214111\n",
      "The semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in a semples in \n",
      "Iter 359. \n",
      "    Loss 5496.10791015625. Time 9.69848108291626\n",
      "The stach of the arthes the sempers and the ardint the stach of the arthes the sempers and the ardint the artimitation the ardint the arthes the sempers and the ardint the artimitation the ardint the arthes the sempers and the ardint the stach of the arthes the sempers and the ardint the stach of the\n",
      "Iter 399. \n",
      "    Loss 5366.49951171875. Time 9.36028790473938\n",
      "The convex of the architectures the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the networks the netwo\n",
      "Iter 439. \n",
      "    Loss 5065.80908203125. Time 9.291939973831177\n",
      "The convex on the propose a neural networks (DNN) hove algorithm that the propose a neural networks (DNN) hove algorithm that the propose a neural networks (DNN) hove algorithm that the propose a neural networks (DNN) hove algorithm that the propose a neural networks (DNN) and the propose a network a\n",
      "Iter 479. \n",
      "    Loss 4466.154296875. Time 9.375052213668823\n",
      "The and the architectures the stace of the architectures the stace of the architectures the stace of the architectures the stace of a semper of the architectures the stace of the architectures the stace of the architectures the stace of the architectures the stace of the architectures the stace of a \n",
      "Iter 519. \n",
      "    Loss 4683.681640625. Time 9.492329120635986\n",
      "The computational convolutional neural networks the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexition the convexiti\n",
      "Iter 559. \n",
      "    Loss 5392.046875. Time 9.384457111358643\n",
      "The and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the successful and the succ\n",
      "Iter 599. \n",
      "    Loss 4513.09375. Time 9.533656120300293\n",
      "The proposed methods that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that the proposed models that \n",
      "Iter 639. \n",
      "    Loss 3960.01611328125. Time 10.173021078109741\n",
      "The architecture and strackgng strategy, stricage and stacked methods on a simple and the computation of the arount of the convergence of the convergence of the convergence of the convergence of the convergence of the convergence of the convergence of the convergence of the convergence of the converg\n",
      "Iter 679. \n",
      "    Loss 4184.1611328125. Time 9.58717393875122\n",
      "The and the state-of-the-art network (DNN) model to a simple training data be training deep learning and machine learning and machine learning and machine learning and machine learning and machine learning and machine learning and machine learning and machine learning and machine learning and machine\n",
      "Iter 719. \n",
      "    Loss 3616.4365234375. Time 10.037940979003906\n",
      "The proposed by the network architectures that aly learning and a new present algorithms to algorithm is popling and the recently for deep neural networks (DNNs) as a contral computation of the architecture of the architecture of the architecture of the architecture of the architecture of the archite\n",
      "Iter 759. \n",
      "    Loss 3972.03125. Time 9.740839004516602\n",
      "The context of the deep learning to compation layer be context of the deep learning to compation layer be context of the deep learning to compation layer be context of the deep learning to compation layer be context of the deep learning to compation layer be context of the deep learning to compation \n",
      "Iter 799. \n",
      "    Loss 4061.370361328125. Time 9.777064085006714\n",
      "The proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed by the proposed \n",
      "Iter 839. \n",
      "    Loss 3480.681640625. Time 9.354095935821533\n",
      "The convex to be the conventional particular training and in the conventional particular training and in the conventional particular training and the conventional particular training and in the conventional particular training and in the conventional particular training and in the conventional partic\n",
      "Iter 879. \n",
      "    Loss 3412.994140625. Time 10.034937143325806\n",
      "The composed learning the the deep learning models and the structure of the signals of the state-of-the-art methods of the state-of-the-art methods of the state-of-the-art methods of the state-of-the-art methods of the state-of-the-art methods of the state-of-the-art methods of the state-of-the-art m\n",
      "Iter 919. \n",
      "    Loss 3351.548583984375. Time 11.192754030227661\n",
      "The structure of a method for standard and a simple such as a methad for the structure of a neural networks (DNNs) are structure of a method for standard and can be the structure of a method for standard and can be the structure of a method for standard and a simple such as a methad for the structure\n",
      "Iter 959. \n",
      "    Loss 3651.00634765625. Time 10.026856184005737\n",
      "The convergence a single layers, such as a new and deep networks and a single layers, such as a new architecture computation in the convergence a new state-of-the-art deep networks and a single layers, such as a new architecture computation in the convergence a new state-of-the-art deep networks and \n",
      "Iter 999. \n",
      "    Loss 3280.296875. Time 9.514609098434448\n",
      "The proposed RNN activation of the and maximint of the experiments of the encoding approximation and maximint of the encoder and the expenseves of large matrix and maximint of the encoder and the expenseves of large matrix and maximint of the encoder and the expenseves of large matrix and maximint of\n",
      "Iter 1039. \n",
      "    Loss 3464.905517578125. Time 10.57132601737976\n",
      "The proposed by the networks are achieved an approach as a problem contrasting the networks are achieved an approach as a problem contrasting the networks are achieved an approach as a problem contrasting the networks are achieved an approach as a problem contrasting the networks are achieved an appr\n",
      "Iter 1079. \n",
      "    Loss 3279.84912109375. Time 10.488875150680542\n",
      "The computation in a general provires and stochastic gradient descent that have been stochastic gradient descent that have been stochastic gradient descent that have been stochastic gradient descent that have been stochastic gradient descent that have been stochastic gradient descent that have been s\n",
      "Iter 1119. \n",
      "    Loss 3591.392578125. Time 9.861019849777222\n",
      "The proposed RNN activation of computational results such as and training a deep networks that a perform deep networks that a deep learning algorithms to the computational computationally interactions and training a deep networks that a perform deep networks that a deep learning algorithms to the com\n",
      "Iter 1159. \n",
      "    Loss 3019.88623046875. Time 9.833359956741333\n",
      "The proposed method for the networks architectures that implemented by the networks architectures that implemented by the networks architectures that implemented by the networks architectures that implemented by the networks architectures that implemented by the networks architectures that implemente\n",
      "Iter 1199. \n",
      "    Loss 3703.96875. Time 10.460994005203247\n",
      "The success of the supervised learning and the success of the significantly initialization to structure of the simple difference by a simple and the supervised learning and the success of the significantly initialization to structure of the simple difference by a simple and the supervised learning an\n",
      "Iter 1239. \n",
      "    Loss 3251.5556640625. Time 10.126414060592651\n",
      "The speech recognition systems with the effect of show that the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of the architecture of t\n",
      "Iter 1279. \n",
      "    Loss 3297.746826171875. Time 10.904850959777832\n",
      "The complex models and the computation of a new previous structure of a deep neural networks to train a deep neural networks to train a deep neural networks to train a deep neural networks to train a deep neural networks to train a deep neural networks to train a deep neural networks to train a deep \n",
      "Iter 1319. \n",
      "    Loss 3234.0458984375. Time 12.183143138885498\n",
      "The proposed RNN accuraces that high predictive in the context of DNN and providing deep neural networks (DNN) have be embedding of the experiments of comprisented to be reduced frameworks for the existence of the training and the experiments of comprisented to be reduced frameworks for the existence\n",
      "Iter 1359. \n",
      "    Loss 2847.193359375. Time 13.160531997680664\n",
      "The framework allows for simple models as way to marg the network accuraing state-of-the-art results such as sperifuc systems are achieved by the architecture of the hierarchy the network is a fired of the architectures contrastic gradient descent can be matrin functions that full architectures contr\n",
      "Iter 1399. \n",
      "    Loss 3094.704833984375. Time 12.18460488319397\n",
      "The conventional neural networks (RNNs) have been well as the network depth discriminative provide the network depth and in a deep neural networks (RNNs) have been well as the network depth discriminative provide the network depth and in a deep neural networks (RNNs) have been well as the network dep\n",
      "Iter 1439. \n",
      "    Loss 3334.28759765625. Time 11.452474117279053\n",
      "The proposed mechanism to computation of an and task derivel and training and matrix computational results on a standard and providing computational results such as constraints and the actorsas between machine learning algorithm to a similary state-of-the-art performance on a connectionase and task o\n",
      "Iter 1479. \n",
      "    Loss 3038.09033203125. Time 11.343908071517944\n",
      "The proposed models are able to iterative models and achieved by a parallel complex models in the main reases to the model as a constrained algorithms to pre-training deep neural networks. We show that the main require state-of-the-art method domain adaptive algorithms to bo the algorithm is computat\n",
      "Iter 1519. \n",
      "    Loss 3137.595458984375. Time 12.443069219589233\n",
      "The context of a single matrix are abolt the achieved by computer vision. We istrate the parameters and the state-of-the-art network size of the state-of-the-art network size of the state-of-the-art network size of the state-of-the-art network size of the state-of-the-art network size of the state-of\n",
      "Iter 1559. \n",
      "    Loss 2998.7705078125. Time 10.603459119796753\n",
      "The proposed $L_p$ unit is states inputanize the successfol systems of the successfol intermations that the full model of the network is a similar speech recognition system is evaluate set of experiments only a simple and can be used a simple of the proposed $L_p$ unit is states inputanize the succes\n",
      "Iter 1599. \n",
      "    Loss 3142.78759765625. Time 10.328867197036743\n",
      "The computation of deep neural networks and in a single layers of adversarial samples by defining a parameter can be a simple problems with a constrained performance in the computation of deep neural networks and in a single layers of adversarial samples by defining a parameter can be a simple proble\n",
      "Iter 1639. \n",
      "    Loss 2744.91845703125. Time 10.029941082000732\n",
      "The for deep neural networks (RNNs) have long been the effectiveness of pre-nample to a simple an approximation eary pooling operating them to sequence of the input data and the state-of-the-art discrimint of the encoder and the input of the training and the experiments of the input data and the stat\n",
      "Iter 1679. \n",
      "    Loss 2952.539794921875. Time 9.90009617805481\n",
      "The framework can be used to form the deep learning mechanisms and relationship between machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine learning machine l\n",
      "Iter 1719. \n",
      "    Loss 2657.75830078125. Time 9.520887851715088\n",
      "The convex optimization and a method is to the network depth in computational results structure of the network depth in computational results structure of the network depth in computational results structure of the network depth in computational results structure of the network depth in computational\n",
      "Iter 1759. \n",
      "    Loss 3082.734619140625. Time 9.359430074691772\n",
      "The computational results suggest that in the each possion function to the setarithming in the computing for the recently interactive MNIST and CIFAR-100 and maxomed results suggest that in the each possion function to the setarithming in the computing for the recently interactive MNIST and CIFAR-100\n",
      "Iter 1799. \n",
      "    Loss 2849.23828125. Time 9.396647930145264\n",
      "The first or also provide a new representations for features and such that the complex optimization framework for unsupervised learning problems to depend neural networks. We show that the main results on a similar recognition involyes to the main recently istradies and the network with networks achi\n",
      "Iter 1839. \n",
      "    Loss 2697.5771484375. Time 9.463186025619507\n",
      "The convex optimization of the network depth and stochastic gradient descent (SGD); but their speed using approach to a simple consisule it is in the context of nonconvex optimization of the network ser as is a simple an approximation of a new are some sules of a single model as a feature for sequenc\n",
      "Iter 1879. \n",
      "    Loss 3011.3818359375. Time 9.40328598022461\n",
      "The algorithm is to train deep network achieved an a similar and can be used as a simple problems with a simple problems in the layers to the state-of-the-art methods as well as a consider an its in the algorithm is to train deep network achieved an a similar and can be used as a simple problems with\n",
      "Iter 1919. \n",
      "    Loss 2963.88623046875. Time 9.502217769622803\n",
      "The form of an expentive functions that computation of the input data on the parameterization of the proposed by the teep solverations in the approach in precisely particul related to the resulting the resulting the approach in precisely paralest at the parameterization of the proposed by the activit\n",
      "Iter 1959. \n",
      "    Loss 2779.27490234375. Time 9.577493906021118\n",
      "The computation in the context of Results on the training between the data visial which was are intimate abbersobal which we present a convexity imare-scale of the training between than full gradient descent (RFN) have been shown to be trained using stochastic gradient descent (RF) and interactive fu\n",
      "Iter 1999. \n",
      "    Loss 2890.68408203125. Time 9.67244291305542\n",
      "The matrix speed up s muptivess functions (2) learning and relationship between variat restardard RNN architectures and maxout networks. We show that the main required seri-targending that the hidden layer, the state-of-the-art deep networks (DNNs) have the networks (DNN) to pre-irility proposed meth\n",
      "Iter 2039. \n",
      "    Loss 2704.797607421875. Time 9.44700002670288\n",
      "The convolutional neural networks are computational results on the network depth and introduce a new framework conventional layer. Tor deep RNNs are complex miniblemm, complicated princupled to a deep neural network complexity of such as deep neural networks and standard networks with a consiruce are\n",
      "Iter 2079. \n",
      "    Loss 3005.181640625. Time 10.075958013534546\n",
      "The for designed to the size of the layer-wise predictions is the loss functions of the log-likelihood rates of layer of the layer-wise predictive recognition structure of the layer-wise predictions is the loss functions of the log-likelihood rates of layer of the layer-wise predictive recognition st\n",
      "Iter 2119. \n",
      "    Loss 2712.072021484375. Time 9.72822904586792\n",
      "The framework of the state of the art on the activations of state-of-the-art performance of state-of-the-art performance of state-of-the-art performance of state-of-the-art performance of state-of-the-art performance of state-of-the-art performance of state-of-the-art performance of state-of-the-art \n",
      "Iter 2159. \n",
      "    Loss 2763.70263671875. Time 9.814518213272095\n",
      "The complexity of speedup to a convex optimization in a deep network to a single machine learning deep neural networks to be reduct the deep learning algorithm from the input fixed bit deep networks with the network independent of the network depth and introduce a new functions that repeat the convex\n",
      "Iter 2199. \n",
      "    Loss 2908.391845703125. Time 9.398157835006714\n",
      "The proposed RNNs classification approaches as the logal convergence to the deep networks in the condextones the layers to durent deep networks in the originally intentice functions that can be achieved competitive with state-of-the-art performance on batch normaccial to the and experimentally intens\n",
      "Iter 2239. \n",
      "    Loss 2924.978759765625. Time 9.664590835571289\n",
      "The formal functions estimate a single machine to form learning rate and the optimization process one show that the model by a framework for deep neural networks (DNNs) are show that the reasons for the inference in the approach for the output of the main result introduced analysis of the network are\n",
      "Iter 2279. \n",
      "    Loss 2582.01123046875. Time 9.480309009552002\n",
      "The context of non-convex optimization, and the conventional layer of the network is an approach is evabulized on many stochastic gradient descent that are introduces and in a rand mole acceleration of the training of computation effective deep networks in machine learning parameters that are large n\n",
      "Iter 2319. \n",
      "    Loss 2782.296875. Time 9.463820934295654\n",
      "The extensive efficiently complex type decemely on a stream algorithm to be model that our algorithm is competitive with such as an application of computational connections and the same to transfer approximate the structurk, we explore the model structure of decayno methods such as RNN activation of \n",
      "Iter 2359. \n",
      "    Loss 2596.79345703125. Time 9.584566116333008\n",
      "The computation of the input data standard between inforeation error rate architectures by defining a deep neural networks for standard RNNs is an autoencoder to a pooling operation of a new pre-training structure, the resulting in strouthing, and a low-dimengs of explain the reconstriction in the co\n",
      "Iter 2399. \n",
      "    Loss 2736.1982421875. Time 9.405165910720825\n",
      "The constrained problem control in computer vision to an a significant results of a standard and randomly representation and maxopting and stacked RNNs in the interpreted a simple strategy popentially, which are successful algorithm is to improve the gradient descent (RNN) to solves different machine\n",
      "Iter 2439. \n",
      "    Loss 2775.845458984375. Time 9.502462148666382\n",
      "The features are in the set iterate. This be implemented by the state-of-the-art performance of the state-of-the-art performance of state-of-the-art performance of the state-of-the-art performance of state-of-the-art performance of the state-of-the-art performance of state-of-the-art performance of t\n",
      "Iter 2479. \n",
      "    Loss 2568.33935546875. Time 9.434529066085815\n",
      "The computational complexity in a dieval network to predict the intimately in a deep network architectures are demonstrating promising approach to be deep learning architectures and layer-wise pre-training structure that it is comment to a mangin approximation of the deep learning architectures and l\n",
      "Iter 2519. \n",
      "    Loss 2321.4541015625. Time 9.453305006027222\n",
      "The ficel starting the neural networks (RNN) as the log of the local canalognts from the log of the hidden factors for training problems for the existence of the input datasets to laree dataset different computational resuling the model and insight from the input of the learned gradient RNNs and MBN \n",
      "Iter 2559. \n",
      "    Loss 2280.051025390625. Time 9.654753923416138\n",
      "The first standard based on a consining the model is computational computation effectiveness of SVRG for neural networks (DNNs) as the layer-wise approach to feaduperal error unly environments and a features from the input of the network architectures can be represented by a first of the computation \n",
      "Iter 2599. \n",
      "    Loss 2587.05126953125. Time 10.916785955429077\n",
      "The convergence rates. We provides the context of nonconvex obtained by computation of the input significanalyzes of the network provides and introduce and stochastic gradient descent computes variance reduction that is deserves subs, we provide to a simple increases and successful the context of non\n",
      "Iter 2639. \n",
      "    Loss 2348.43798828125. Time 9.882039070129395\n",
      "The extension foress to adversarial setting, on a novel deep learning algorithm for context of neural network (DNN) model condectone approximation and output dataset set of experiments learning algorithm for context of learning algorithm from a state-of-the-art minibatch sore, providual preserving an\n",
      "Iter 2679. \n",
      "    Loss 2493.30712890625. Time 10.300896883010864\n",
      "The functions that can be trained to explore the model in computational y enderstonal performance on a single machine learning in computational results on the model is computational y ensel networks are a new pre-training a construct of a single method for training and a single matrix and the propose\n",
      "Iter 2719. \n",
      "    Loss 2407.596435546875. Time 9.790703058242798\n",
      "The convergence to study the convex ERMs in WER on the convex optimization processing problems (or interaction of a connection with the interpreter approach in speech recognition to an optimile in an important for distribution from the learning problems (or intererties that can be trained the convex \n",
      "Iter 2759. \n",
      "    Loss 2277.36328125. Time 11.367195129394531\n",
      "The proposed $L_p$ unit defined by the neep network are simple and structure of the network. Specinize and maxout unit recognition of the successive algorithm fromingmes that the network, applications when thet we show the existence of {\\em simplest distingate method for existing results on the scale\n",
      "Iter 2799. \n",
      "    Loss 2201.6416015625. Time 9.814318895339966\n",
      "The complex models. Experiment set of under to architecture computation accuracy. We also constrained with a constraintance multiel paper, we propose a method for the out unit (RNN) acoustic models have yielded training algorithms such as deep neural networks that computation of the input data. This \n",
      "Iter 2839. \n",
      "    Loss 2617.248046875. Time 12.158195972442627\n",
      "The first several parameters to support the standard RNN architecture complex types of the network depth and on the firs using the log-likelihood and the propertion and many set of support disting recognition to a simple supportidized for training techniques allows network into a single model for the\n",
      "Iter 2879. \n",
      "    Loss 2333.744140625. Time 10.98099398612976\n",
      "The machine learning and we propose a neural networks for each neural networks (DNNs) are not achieve the encore functions using an empirical inverse them to estimate the machines when to be reduced as a set of experiments using a new framework cal convergence rates for the input of seme-norm that dr\n",
      "Iter 2919. \n",
      "    Loss 2443.634521484375. Time 11.604578971862793\n",
      "The conventional layer-wise methods and deep neural network (DNN) model and understanding the computation of the network depth, thereby encoding theoretical results on a deep network convergence to a both between machines of the network depth, thereby encoding theoretical results on a deep network co\n",
      "Iter 2959. \n",
      "    Loss 2367.39501953125. Time 10.191084861755371\n",
      "The extensively studied in a deep retained analysis of deep networks on different distribution factorization and the number of large-scale deep networks on different distribution factorization and the number of large-scale deep networks on different distribution factorization and the number of large-\n",
      "Iter 2999. \n",
      "    Loss 1924.750244140625. Time 10.597562074661255\n",
      "The features for training deep neural networks (DNNs) as a plyor to the set of be maxout dimans in recent calce acher to ply ain a pre-imporsing a cluster of the number of limited training algorithm is computation of the success of deep neural networks (DNNs) as a plyor to the set of be maxout dimans\n",
      "Iter 3039. \n",
      "    Loss 2437.60107421875. Time 10.76187801361084\n",
      "The computation involvainatively approximate itherecestruction a log-distriby with number of marimessive iterations that can use them the computational complexity of the network pruning and algorithmsise such as images, are them successfor approximation of the network is not extent of computation is \n",
      "Iter 3079. \n",
      "    Loss 2392.6669921875. Time 11.576658010482788\n",
      "The backpropagation, which achieve the pre-training stochastic gradient descent. We introduce a setter countermation and the pre-training the state-of-the-art networks (RNNs) have benenso a simple from a regurk ondised method using a new approach to train data standard RNN actor that called in a deep\n",
      "Iter 3119. \n",
      "    Loss 2277.92431640625. Time 11.207668781280518\n",
      "The proposed $L_p$ unit in test to a single layers court expensive error the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the input of the inp\n",
      "Iter 3159. \n",
      "    Loss 2531.1357421875. Time 10.122661113739014\n",
      "The first or a single for generalization to one of the input data state-of-the-art neural networks is a context and network is is is used for Quantited for significantly improvements in many different distributed from previous previously though To model in a single layer from sontand speech recogniti\n",
      "Iter 3199. \n",
      "    Loss 2246.06884765625. Time 10.44557499885559\n",
      "The benefits of our algorithm is a single learning the architectures are graphs of the success of deep neural networks (DNNs) acoustic models for each tomerate the state-of-the-art methods seffer framework for each takens. Our implementations of convex ERMs. ERchithore algorithm can be are also expen\n",
      "Iter 3239. \n",
      "    Loss 2300.40478515625. Time 11.171653985977173\n",
      "The computation with perform learning problems and deep learning algorithms is often performed by computational complex to an effective mechanisms and deep neural networks and show that the deep RNNs can be man parameter learning architecture based on the proposed different data. This deep learning a\n",
      "Iter 3279. \n",
      "    Loss 2372.80029296875. Time 9.735771894454956\n",
      "The learning algorithm to be a simple function models have benefted classification processing layers and the state-of-the-art databound by advantage us to the ovirated by the training deep neural networks are and compressionely is a simple models by the difficult, with state-of-the-art derierys show \n",
      "Iter 3319. \n",
      "    Loss 2375.53662109375. Time 10.773302793502808\n",
      "The model accuracy of the abolt deep learning algorithms for stochastic rounding the state-of-the-art on a deep neural network (DNN) model first. Network provides a gradient performance of semarates at the state-of-the-art on a deep neural network (DNN) model first. Network provides a gradient perfor\n",
      "Iter 3359. \n",
      "    Loss 2177.51025390625. Time 10.20835018157959\n",
      "The conventional networks that can be well on maximam belowed problem. The number of processing datasets, we conventing the new approach the neural networks that can be well on the conventional neural networks that can be well on the conventional neural networks that can be well on the conventional n\n",
      "Iter 3399. \n",
      "    Loss 2181.88037109375. Time 10.11763596534729\n",
      "The benefits of our experiments on a diffeeen meansul approaches are gaid and relevant interesting function when that incroduced to train the effectiveness and endigining networks (DNN) hodely dominasting on a deep convolutional and ensionable to address the value function and there is to increase in\n",
      "Iter 3439. \n",
      "    Loss 2111.29833984375. Time 10.135020017623901\n",
      "The complex input of the input data stating the state-of-the-art results on a deep neural network (DNN) has well as max-pooling state-of-the-art results on a deep neural networks (DNNs) as a previous with the information in task for the output of a method for trans one of the applied on which takes o\n",
      "Iter 3479. \n",
      "    Loss 2219.572021484375. Time 10.091590881347656\n",
      "This paper preserving essentially with nondayion limetrations (incoppsing neural networks (RNNs) have results in parallel speech recognition in the convergence a setting that is into ensemble formulation of the network prunidge cormus on in improvements in the context of neural networks are able to a\n",
      "Iter 3519. \n",
      "    Loss 2147.2900390625. Time 10.207274198532104\n",
      "This paper proposes a generalized and the state-of-the-art deep networks that are activations that are activations that are activations that are activations that are activations that are activations that are activations that are activations that are activations that are activations that are activatio\n",
      "Iter 3559. \n",
      "    Loss 2293.259765625. Time 9.944908857345581\n",
      "The computation of computational result and a training capation to possing are individual neural networks (RNNs) arematrices of the network are likely to pre-train new architectures are required to pre-train very stod-aut method to pre-train very stod-Convidule predictions in poiting the accuracy of \n",
      "Iter 3599. \n",
      "    Loss 2137.78564453125. Time 10.592572927474976\n",
      "The experiments levend neural network depth and show that but also datasets are simple vision of non-negative machine learning task in a random factiven which make provided on the convergence rates in MBN. In this paper, we consider the data stability of the same from man ba different fin losted data\n",
      "Iter 3639. \n",
      "    Loss 2172.26171875. Time 10.116750001907349\n",
      "The experiments lod-up, the variance of the recently invessization achieving and Adamole, all ways the tasks of the amount of computational of--heral eary that can be crastic generator with minimal orbits and the standard units and a transformation). We show that the main results on the same prinfiru\n",
      "Iter 3679. \n",
      "    Loss 2245.429931640625. Time 10.051018953323364\n",
      "The computational complexity of the network is not extree to a contras into the context oftroblem convolutional networks, which ortence reduction of the network is not extrees to better priccipuly previous to computation of feedforward neural networks are able to adversarial samples: incurty the comp\n",
      "Iter 3719. \n",
      "    Loss 2198.57763671875. Time 9.943665027618408\n",
      "The benefits of our models. We used to better speed on standard bounds on the convergence rates of binating algorithms and convexity and strication, but also implementing baskpropagation to a digtien in the advances have bonest and therefore doons in the logarization and neural networks (RNN) accurac\n",
      "Iter 3759. \n",
      "    Loss 2334.50439453125. Time 9.727980852127075\n",
      "The resulting entry alownet flaits the standard approach to fead of the same propose the importance of the success of deep learning algorithms to caperaie the state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art resul\n",
      "Iter 3799. \n",
      "    Loss 2187.0859375. Time 10.104104042053223\n",
      "The convergence rates of the network depth into Simractions. In particular, outperforms with non-convex experiments on a stream of the network depth and intentical information and deep networks in machine learning the activation functions with no previously known models using stochastic gradient desc\n",
      "Iter 3839. \n",
      "    Loss 2143.18359375. Time 9.587104797363281\n",
      "The benefits and may both be presenve to the model structure of deep neural networks (RNNs) have becent descent to tradsem of stage. On the main domain to address the distributed computationally units of the DNNs. We also explore hand neep that the prediction to the scale of under transition Fields (\n",
      "Iter 3879. \n",
      "    Loss 2268.62060546875. Time 10.62985897064209\n",
      "The benefits and understanding the size of existing deep learning architectures by a fram extond a simple to the network is an ameler networks the reashnized by the nearially and explains the same tasks and show that the properties of a new method impressed depth, the network are large novel algorith\n",
      "Iter 3919. \n",
      "    Loss 2080.20556640625. Time 10.064941883087158\n",
      "The large-scale proposed method can be tor seed and distribution provided can be achieved in an important provident to the conventional deependent to intermution pathwarch is estimation of non-convex optiminative Stoccaser specific to studied by the acountion or multiple results on MNIST, CIFAR-10, w\n",
      "Iter 3959. \n",
      "    Loss 2283.565185546875. Time 10.384665966033936\n",
      "This paper proposes algorithm for context of persuined with ne reduction can be used to based on the transformations of this state-of-the-art on MNIST, CIFAR-10 and systems with the state-of-the-art machines of the input data state-of-the-art results on MNIST, CIFAR-10 graphs). This paper, we explore\n",
      "Iter 3999. \n",
      "    Loss 2046.001708984375. Time 9.759718179702759\n",
      "This paper proposes an a random walk on the been the context procedure. We show that the deep learning architectures can be are able to computation of the architecture computational result of the context problem to an ither to decode the approximate interpretation of the hierarchy. The logal such get\n",
      "Iter 4039. \n",
      "    Loss 2089.825439453125. Time 10.236252069473267\n",
      "This layer and output of the parameters are non-negalipating gever learner directly reducing the number of layers and the context of non-convex optimization problem involving a deeper layers, can be achieved in many different algorithms such as training layers and the proposed methods for the reasons\n",
      "Iter 4079. \n",
      "    Loss 2289.44140625. Time 10.44957709312439\n",
      "The model as a method for trans of deep learning algorithms is often initialize a new state-of-the-art systems without a classification approaches to form a more of the approach for desirable propose a new state-of-the-art DNN and RNN model frem the system is an approach for existing size and stacked\n",
      "Iter 4119. \n",
      "    Loss 2169.45361328125. Time 9.93993091583252\n",
      "The computational configurations, and provide a complex mapping them powerful functions that the network pruning using general networks to explore a method to inference it as images for deep neural networks that arem to learn the model structure of the network pruning is a simple considues the convex\n",
      "Iter 4159. \n",
      "    Loss 1915.5743408203125. Time 10.297691106796265\n",
      "The experiments use the difficulties of convex ERMs and show that fully consimplimitivizative maxrup challengized stabels is machine learning to maximally preserve DBN on the contrast to the deeper layers, can be achieved in many classification approaches to focusive differentiod and the current deep\n",
      "Iter 4199. \n",
      "    Loss 1901.5860595703125. Time 10.2014479637146\n",
      "The benchmark datasets that realing methods is an aumoencoders. The pre-training structure-d-semi-non^wing the complex mapping frameworks. However, the reasons for regularizer -artien by units in the non-current batches require that increasing iterative models and show that batches and computation of\n",
      "Iter 4239. \n",
      "    Loss 2131.2451171875. Time 9.867324113845825\n",
      "The complexity of MBN is existing preserved digense-bound iterations. In this paper, we present a new preservation is essentially the nots of the acoustic sourcesssur results on MNIST and we show that we sugmettodifned is using the network problems with little in the computational complexity results \n",
      "Iter 4279. \n",
      "    Loss 2003.55078125. Time 9.710839986801147\n",
      "The expense on different fined-briak boling, approach to eltions as a backpropagation of an approximation error. By when applying the state-of-the-art on challenge in the $L_p$ unit. Our a Deep relative to train objective to the inference in the maintlic optimization of these deep RNN using a novel g\n",
      "Iter 4319. \n",
      "    Loss 2039.4365234375. Time 11.00193190574646\n",
      "The computation of the input. Simelariant and litalate methods to adversarial size in teed problems invessigate the network is average to allow pooling to learn $\\lambdains. In particular, why the abtorm to use pre-training signals in trains the model structure, show that their neural networks. We th\n",
      "Iter 4359. \n",
      "    Loss 2056.150390625. Time 10.242733001708984\n",
      "The information factorization and our pra-train an imputation for the local optimazarized significant reducing the success of our pre-training and strides when convolutional neural network distributing the architecture can be are able to better reletagithms for significantly improvements in computer \n",
      "Iter 4399. \n",
      "    Loss 2010.7130126953125. Time 10.250810146331787\n",
      "The experiments love a simple for gradient descent the experiments love a new training of complex one of the speech recognized word error rates to training on a general for learning rates. The introduced as a generalized on the same proposed deep shalle proposed method using a conventation---------wi\n",
      "Iter 4439. \n",
      "    Loss 1874.96240234375. Time 10.590472936630249\n",
      "The complexity in several deep learning algorithms have been the context from ustention units and may provide parametric itformet of large dataseted and previous pooling applications. The standard Rprop howevel type of expertsest to explore themselved deep neural networks (DNNs) as submetting the ran\n",
      "Iter 4479. \n",
      "    Loss 1955.3369140625. Time 10.771372079849243\n",
      "The experiments dese the output  approach for distillation to an ideal neural network architectures by a large data. An and test them the proposed methods for stochastic gradient descent (SGD), in terms of computationally expensive to and one of the input dimensify the pre-training methods. The state\n",
      "Iter 4519. \n",
      "    Loss 1909.904052734375. Time 10.618738889694214\n",
      "The first eachent-of the state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "DATA_PATH = './arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "LR = 0.003\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "LEN_GENERATED = 300\n",
    "TEMPRATURE = 0.7\n",
    "\n",
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.core_rnn_cell.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "        cell.zero_state(tf.shape(seq)[0], tf.float32),[None, hidden_size])\n",
    "    \n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, output_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, output_state\n",
    "\n",
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:,:-1], labels=seq[:,1:]))\n",
    "    \n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0]\n",
    "    return loss, sample, in_state, out_state\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(arr, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in arr])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS/2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, int(overlap)):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "        \n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch\n",
    "    \n",
    "\n",
    "def online_interface(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        \n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)\n",
    "    \n",
    "def trainning(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_interface(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                \n",
    "                if not os.path.exists('checkpoints/arvix/char-rnn'): os.makedirs('checkpoints/arvix/char-rnn')\n",
    "                \n",
    "                saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "    \n",
    "\n",
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    \n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='globa_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    trainning(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Language Modeling\n",
    "* センテンスがどの程度似ているかを測定できる\n",
    "* 機械翻訳にとって重要なinputである\n",
    "* 新規の文書を作成できる\n",
    "\n",
    "### 主なアプローチ\n",
    "* 単語レベル(n-gram)\n",
    "    * 伝統的なやり方\n",
    "    * 前のn-gramにもとづいて次のwordを予測するためにモデルを学習する\n",
    "    * 多くの語彙を必要とする\n",
    "    * Out Of Vocabularyが発生しない\n",
    "    * 大量のメモリが必要\n",
    "* 文字レベル\n",
    "    * 入力も出力も文字である\n",
    "    *  利点\n",
    "        * 語彙が少なくても良い\n",
    "        *  word embeddings が不要\n",
    "        * 訓練が高速\n",
    "    * 欠点\n",
    "        * 言葉がちんぷんかんぷんなものができる\n",
    "    * Unknownなものだけ文字レベルに切り替える方法もある\n",
    "* subword(音声)レベル\n",
    "    * inputもoutputもsubwords\n",
    "    * W を最も頻出する単語とする\n",
    "    * S を最も頻出する音節とする\n",
    "    * それらを文字に分割する\n",
    "    * wordレベルやcharactorレベルのモデルよりもよく動いているように見える"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
